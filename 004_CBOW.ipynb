{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsB07AlBk9mNxPXbvcPrKt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deguc/Shannon/blob/main/004_CBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGvgGrC7RVSi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def onehot(x,k):\n",
        "\n",
        "    return np.identity(k)[x]\n",
        "\n",
        "\n",
        "def dataset(token,size,vocab_size):\n",
        "\n",
        "    X,Y = [],[]\n",
        "    k = len(token)\n",
        "\n",
        "    for i in range(k-size):\n",
        "\n",
        "        j = i+size\n",
        "\n",
        "        X += [token[i:j]]\n",
        "        Y += [token[j]]\n",
        "\n",
        "    return np.vstack(X),onehot(np.hstack(Y),vocab_size)\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "\n",
        "    def __init__(self,dataset,batch_size=10):\n",
        "\n",
        "        self.x,self.y = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.data_size = self.x.shape[0]\n",
        "        self.cnt = 0\n",
        "\n",
        "    def shuffle(self):\n",
        "\n",
        "        idx = np.random.permutation(self.data_size)\n",
        "        self.x,self.y = self.x[idx],self.y[idx]\n",
        "\n",
        "    def get_idx(self,idx):\n",
        "\n",
        "        i = self.batch_size*idx\n",
        "        j = i + self.batch_size\n",
        "\n",
        "        return i,j\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size // self.batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "\n",
        "        if idx < 0 or idx >= len(self):\n",
        "            raise IndexError('out of range')\n",
        "\n",
        "        i,j = self.get_idx(idx)\n",
        "\n",
        "        return self.x[i:j],self.y[i:j]\n",
        "\n",
        "    def __next__(self):\n",
        "\n",
        "        if self.cnt == 0:\n",
        "            self.shuffle()\n",
        "\n",
        "        if self.cnt >= len(self):\n",
        "\n",
        "            self.cnt = 0\n",
        "\n",
        "            raise StopIteration\n",
        "\n",
        "        else:\n",
        "\n",
        "            i,j = self.get_idx(self.cnt)\n",
        "\n",
        "            self.cnt += 1\n",
        "\n",
        "            return self.x[i:j],self.y[i:j]\n",
        "\n",
        "\n",
        "def zeros_ps(ps):\n",
        "\n",
        "    gs = []\n",
        "\n",
        "    for p in ps:\n",
        "\n",
        "        gs += [np.zeros_like(p)]\n",
        "\n",
        "    return gs\n",
        "\n",
        "\n",
        "class Module:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.ps,self.gs = [],[]\n",
        "        self.train_flag = None\n",
        "\n",
        "\n",
        "class Linear(Module):\n",
        "\n",
        "    def __init__(self,d_in,d_out,biased=True):\n",
        "        super().__init__()\n",
        "        self.biased = biased\n",
        "\n",
        "        std = np.sqrt(d_in/2)\n",
        "        self.ps = [\n",
        "            np.random.randn(d_in,d_out)/std,\n",
        "            np.zeros(d_out)\n",
        "        ]\n",
        "        self.gs = zeros_ps(self.ps)\n",
        "\n",
        "        self.inputs = None\n",
        "\n",
        "    def __call__(self,x):\n",
        "\n",
        "        self.inputs = x\n",
        "\n",
        "        return x @ self.ps[0] + self.ps[1]\n",
        "\n",
        "    def backward(self,dout):\n",
        "\n",
        "        self.gs[0][...] = self.inputs.T @ dout\n",
        "\n",
        "        if self.biased:\n",
        "            self.gs[1][...] = np.sum(dout,axis=0)\n",
        "\n",
        "        return dout @ self.ps[0].T\n",
        "\n",
        "\n",
        "class ReLU(Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mask = None\n",
        "\n",
        "    def __call__(self,x):\n",
        "\n",
        "        self.mask = x <= 0\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self,dout):\n",
        "\n",
        "        dout[self.mask] = 0\n",
        "\n",
        "        return dout\n",
        "\n",
        "\n",
        "class CBOW:\n",
        "\n",
        "    def __init__(self,vocab_size,d_emb,ctx_size):\n",
        "\n",
        "        self.ctx_size = ctx_size\n",
        "        std = np.sqrt(vocab_size/2)\n",
        "        W = np.random.randn(vocab_size,d_emb)/std\n",
        "        self.emb = [Embedding(W) for _ in range(ctx_size)]\n",
        "        self.aff = Linear(d_emb*ctx_size,vocab_size,biased=False)\n",
        "        self.layers = [*self.emb,self.aff]\n",
        "\n",
        "        self.ps = [[],[]]\n",
        "\n",
        "        for l in self.layers:\n",
        "            self.ps[0] += l.ps\n",
        "            self.ps[1] += l.gs\n",
        "\n",
        "    def __call__(self,x):\n",
        "\n",
        "        h = []\n",
        "        for i,e in enumerate(self.emb):\n",
        "            h += [e(x[:,i])]\n",
        "\n",
        "        out = np.hstack(h)\n",
        "\n",
        "        return self.aff(out)\n",
        "\n",
        "    def backward(self,dout):\n",
        "\n",
        "\n",
        "        dout = self.aff.backward(dout)\n",
        "        dout = np.split(dout,self.ctx_size,axis=-1)\n",
        "\n",
        "        for e,d in zip(self.emb,dout):\n",
        "            e.backward(d)\n",
        "\n",
        "    def pred(self,x):\n",
        "\n",
        "        return np.argmax(self(x),axis=-1)\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        for l in self.layers:\n",
        "            l.train_flag = True\n",
        "\n",
        "    def eval(self):\n",
        "\n",
        "        for l in self.layers:\n",
        "            l.train_flag = False\n",
        "\n",
        "\n",
        "def softmax(x):\n",
        "\n",
        "    c = np.max(x,axis=-1,keepdims=True)\n",
        "    z = np.exp(x-c)\n",
        "\n",
        "    return z / np.sum(z,axis=-1,keepdims=True)\n",
        "\n",
        "\n",
        "def cross_entropy(y,t):\n",
        "\n",
        "    eps = 1e-6\n",
        "\n",
        "    return -np.sum(t*np.log(y+eps))/y.shape[0]\n",
        "\n",
        "\n",
        "class Loss:\n",
        "\n",
        "    def __init__(self,model,clf=softmax,loss=cross_entropy):\n",
        "\n",
        "        self.model = model\n",
        "        self.clf = clf\n",
        "        self.loss = loss\n",
        "\n",
        "        self.dout = None\n",
        "\n",
        "\n",
        "    def __call__(self,y,t):\n",
        "\n",
        "        out = self.clf(y)\n",
        "        self.dout = out - t\n",
        "\n",
        "        return self.loss(out,t)\n",
        "\n",
        "    def backward(self):\n",
        "        self.model.backward(self.dout)\n",
        "\n",
        "\n",
        "class AdamW:\n",
        "\n",
        "    def __init__(self,ps,lr,beta1=0.25,beta2=0.9,weight_decay=0.1):\n",
        "\n",
        "        self.ps = ps\n",
        "        self.cache = (lr,beta1,beta2,weight_decay)\n",
        "        self.hs = [\n",
        "            zeros_ps(ps[0]),\n",
        "            zeros_ps(ps[1])\n",
        "        ]\n",
        "        self.cnt = 0\n",
        "\n",
        "    def __call__(self):\n",
        "\n",
        "        eps = 1e-6\n",
        "        ps,gs = self.ps\n",
        "        ms,vs = self.hs\n",
        "        lr,b1,b2,w = self.cache\n",
        "        self.cnt += 1\n",
        "        n = self.cnt\n",
        "\n",
        "        for p,g,m,v in zip(ps,gs,ms,vs):\n",
        "\n",
        "            m[...] = b1*m + (1-b1)*g\n",
        "            v[...] = b2*v + (1-b2)*g*g\n",
        "\n",
        "            m0 = m / (1-b1**n)\n",
        "            v0 = v / (1-b2**n)\n",
        "\n",
        "            p - w*lr*g\n",
        "\n",
        "            p -= lr*m0/(np.sqrt(v0)+eps)\n",
        "\n",
        "\n",
        "def trainer(model,loss,optimizer,data,epochs=100):\n",
        "\n",
        "    ls = []\n",
        "\n",
        "    for _ in range(epochs):\n",
        "\n",
        "        model.train()\n",
        "        l = 0\n",
        "\n",
        "        for x,t in data:\n",
        "\n",
        "            y = model(x)\n",
        "            l += loss(y,t)\n",
        "            loss.backward()\n",
        "            optimizer()\n",
        "\n",
        "        ls += [l/len(data)]\n",
        "        model.eval()\n",
        "\n",
        "    return ls\n",
        "\n",
        "\n",
        "def disp_loss(loss):\n",
        "\n",
        "    plt.title('Loss Function')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('cross entropy')\n",
        "    plt.plot(loss)\n",
        "    plt.show()\n",
        "\n",
        "class Tokenizer:\n",
        "\n",
        "    def __init__(self,text):\n",
        "\n",
        "        vocab = {}\n",
        "\n",
        "        for w in text:\n",
        "\n",
        "            if w not in vocab:\n",
        "                vocab[w] = len(vocab)\n",
        "\n",
        "        self.vocab = vocab\n",
        "\n",
        "        self.dic = {k:v for v,k in self.vocab.items()}\n",
        "\n",
        "        self.token = np.array([vocab[w] for w in text])\n",
        "\n",
        "        self.vocab_size = len(vocab)\n",
        "\n",
        "    def encode(self,text):\n",
        "\n",
        "        encoded = []\n",
        "\n",
        "        for w in text:\n",
        "            encoded += [self.vocab[w]]\n",
        "\n",
        "        return encoded\n",
        "\n",
        "    def decode(self,encoded):\n",
        "\n",
        "        decoded = ''\n",
        "\n",
        "        for i in encoded:\n",
        "            decoded += self.dic[i]\n",
        "\n",
        "        return decoded\n",
        "\n",
        "\n",
        "class Embedding(Module):\n",
        "\n",
        "    def __init__(self,W):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ps = [W]\n",
        "        self.gs =zeros_ps(self.ps)\n",
        "\n",
        "        self.idx = None\n",
        "\n",
        "    def __call__(self,idx):\n",
        "\n",
        "        W, = self.ps\n",
        "        self.idx = idx\n",
        "\n",
        "        return W[idx]\n",
        "\n",
        "    def backward(self,dout):\n",
        "\n",
        "        dW, = self.gs\n",
        "        dW[...] = 0\n",
        "        np.add.at(dW,self.idx,dout)\n",
        "\n",
        "def generator(model,tokenizer,text,ctx_size,new_size):\n",
        "\n",
        "    new_token = []\n",
        "    token = tokenizer.encode(text)\n",
        "\n",
        "    for _ in range(new_size):\n",
        "       x = np.array([token[-ctx_size:]])\n",
        "       new_word = model.pred(x)[0]\n",
        "       token += [new_word]\n",
        "       new_token += [new_word]\n",
        "\n",
        "    return tokenizer.decode(new_token)\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2,suppress=True)\n",
        "\n",
        "text = 'これやこの いくもかえるも わかれては しるもしらぬも おうさかのせき'\n",
        "\n",
        "tokenizer = Tokenizer(text)\n",
        "token = tokenizer.token\n",
        "vocab_size = tokenizer.vocab_size\n",
        "d_emb = 4*vocab_size\n",
        "ctx_size = 5\n",
        "\n",
        "dataset = dataset(token,size=ctx_size,vocab_size=vocab_size)\n",
        "data = DataLoader(dataset,batch_size=5)\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "model = CBOW(vocab_size,d_emb,ctx_size)\n",
        "loss = Loss(model)\n",
        "optimizer = AdamW(model.ps,lr=0.01)\n",
        "ls = trainer(model,loss,optimizer,data,epochs)\n",
        "\n",
        "text = 'これやこの'\n",
        "pred = generator(model,tokenizer,text,ctx_size,new_size=8)\n",
        "print(pred)\n",
        "#disp_loss(ls)\n",
        "\n"
      ]
    }
  ]
}