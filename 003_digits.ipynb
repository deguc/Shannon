{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLbvHxVTFH5sGbaWh4ct04",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deguc/Shannon/blob/main/003_digits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "id": "KPdGw6Yhj4SW",
        "outputId": "8b91a24e-6592-49ff-f3ec-595d0887c7ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs = 0 loss = 0.5858407657766533\n",
            "epochs = 10 loss = 0.0015999558227642492\n",
            "epochs = 20 loss = 0.00028148907220317194\n",
            "epochs = 30 loss = 0.00017159834479599746\n",
            "epochs = 40 loss = 4.478709753404834e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.datasets import load_digits\n",
        "\n",
        "def onehot(x,k=10):\n",
        "\n",
        "    return np.identity(k)[x]\n",
        "\n",
        "\n",
        "def dataset(digits,i=0,size=1000):\n",
        "\n",
        "    j = i+size\n",
        "\n",
        "    return digits.images[i:j].reshape(-1,64),onehot(digits.target[i:j],10)\n",
        "\n",
        "\n",
        "class DataLoader:\n",
        "\n",
        "    def __init__(self,dataset,batch_size):\n",
        "\n",
        "        self.x,self.y = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.data_size = self.x.shape[0]\n",
        "        self.cnt = 0\n",
        "\n",
        "    def shuffle(self):\n",
        "\n",
        "        idx = np.random.permutation(self.data_size)\n",
        "        self.x = self.x[idx]\n",
        "        self.y = self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_size // self.batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "\n",
        "        if self.cnt == 0:\n",
        "            self.shuffle()\n",
        "\n",
        "        i = self.cnt\n",
        "        j = i + self.batch_size\n",
        "\n",
        "        if j > self.data_size:\n",
        "\n",
        "            self.cnt = 0\n",
        "            raise StopIteration\n",
        "\n",
        "        else:\n",
        "\n",
        "            self.cnt = j\n",
        "\n",
        "            return self.x[i:j],self.y[i:j]\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "        if idx <0 or idx >= len(self):\n",
        "            raise IndexError('out of index')\n",
        "\n",
        "        i = idx * self.batch_size\n",
        "        j = i + self.batch_size\n",
        "\n",
        "        return self.x[i:j],self.y[i:j]\n",
        "\n",
        "\n",
        "class Module:\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.ps,self.gs = [],[]\n",
        "        self.train_flag = False\n",
        "        self.mask = None\n",
        "        self.inputs = None\n",
        "        self.out = None\n",
        "\n",
        "\n",
        "def zeros_ps(ps):\n",
        "\n",
        "    gs = []\n",
        "\n",
        "    for p in ps:\n",
        "        gs += [np.zeros_like(p)]\n",
        "\n",
        "    return gs\n",
        "\n",
        "\n",
        "class Linear(Module):\n",
        "\n",
        "    def __init__(self,d_in,d_out):\n",
        "        super().__init__()\n",
        "\n",
        "        he = np.sqrt(d_in/2)\n",
        "\n",
        "        self.ps = [\n",
        "            np.random.randn(d_in,d_out)/he,\n",
        "            np.zeros(d_out)\n",
        "        ]\n",
        "        self.gs = zeros_ps(self.ps)\n",
        "\n",
        "    def __call__(self, x):\n",
        "\n",
        "        self.inputs = x.copy()\n",
        "\n",
        "        return x @ self.ps[0] + self.ps[1]\n",
        "\n",
        "    def backward(self, dout):\n",
        "\n",
        "        self.gs[0][...] = self.inputs.T @ dout\n",
        "        self.gs[1][...] = np.sum(dout,axis=0)\n",
        "\n",
        "        return dout @ self.ps[0].T\n",
        "\n",
        "\n",
        "class ReLU(Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def __call__(self, x):\n",
        "\n",
        "        self.mask = x < 0\n",
        "        out = x.copy()\n",
        "        out [self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "\n",
        "        return dout\n",
        "\n",
        "\n",
        "class Dropout(Module):\n",
        "\n",
        "    def __init__(self,drop_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.drop_rate = drop_rate\n",
        "\n",
        "    def __call__(self,x):\n",
        "\n",
        "        if self.train_flag:\n",
        "            r = self.drop_rate\n",
        "        else:\n",
        "            r = 0\n",
        "\n",
        "        self.mask = np.random.rand(*x.shape) > r\n",
        "\n",
        "        return x * self.mask /(1-r)\n",
        "\n",
        "    def backward(self,dout):\n",
        "\n",
        "        return dout * self.mask /(1-self.drop_rate)\n",
        "\n",
        "\n",
        "class BatchNorm(Module):\n",
        "\n",
        "    def __init__(self,dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.ps = [\n",
        "            np.ones(dim),\n",
        "            np.zeros(dim)\n",
        "        ]\n",
        "        self.gs = zeros_ps(self.ps)\n",
        "\n",
        "        self.mu = np.zeros(dim)\n",
        "        self.var = np.ones(dim)\n",
        "\n",
        "        self.cache = None\n",
        "\n",
        "        self.eps = 1e-6\n",
        "\n",
        "    def __call__(self,x):\n",
        "\n",
        "        if self.train_flag:\n",
        "\n",
        "            mu = np.mean(x,axis=0)\n",
        "            var = np.var(x,axis=0)\n",
        "            centered = x-mu\n",
        "            std_inv = 1/np.sqrt(var+self.eps)\n",
        "            norm = centered * std_inv\n",
        "            self.cache = (centered,std_inv,norm)\n",
        "\n",
        "            self.mu[...] = 0.9*self.mu + 0.1 * mu\n",
        "            self.var[...] = 0.9*self.var + 0.1*var\n",
        "\n",
        "        else:\n",
        "            norm = (x-self.mu)/np.sqrt(self.var+self.eps)\n",
        "\n",
        "        return self.ps[0] * norm + self.ps[1]\n",
        "\n",
        "    def backward(self,dout):\n",
        "\n",
        "        centered,std_inv,norm = self.cache\n",
        "\n",
        "        self.gs[0][...] = np.sum(dout*norm,axis=0)\n",
        "        self.gs[1][...] = np.sum(dout,axis=0)\n",
        "\n",
        "        dnorm = self.ps[0] * dout\n",
        "        dvar = -0.5*np.sum(dnorm*centered*std_inv**3,axis=0)\n",
        "        dmu = -np.sum(dnorm*std_inv,axis=0)-2.0*np.mean(dvar*centered)\n",
        "\n",
        "        n = dout.shape[0]\n",
        "\n",
        "        return dnorm*std_inv+2.0*dvar*centered/n+dmu/n\n",
        "\n",
        "\n",
        "def softmax(y,t):\n",
        "\n",
        "    eps = 1e-6\n",
        "    n = y.shape[0]\n",
        "\n",
        "    c = np.max(y,axis=-1,keepdims=True)\n",
        "    e = np.exp(y-c)\n",
        "    Z = np.sum(e,axis=-1,keepdims=True)\n",
        "    out = e/Z\n",
        "\n",
        "    dout = (out-t)/n\n",
        "\n",
        "    loss = -np.sum(t*np.log(out+eps))/n\n",
        "\n",
        "    return dout,loss\n",
        "\n",
        "\n",
        "class Layers:\n",
        "\n",
        "    def __init__(self,layers):\n",
        "\n",
        "        self.layers = layers\n",
        "        self.ps = [[],[]]\n",
        "\n",
        "        for l in self.layers:\n",
        "            self.ps[0] += l.ps\n",
        "            self.ps[1] += l.gs\n",
        "\n",
        "    def __call__(self,x):\n",
        "\n",
        "        for l in self.layers:\n",
        "            x = l(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def backward(self,dout):\n",
        "\n",
        "        for l in reversed(self.layers):\n",
        "            dout = l.backward(dout)\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        for l in self.layers:\n",
        "            l.train_flag = True\n",
        "\n",
        "    def eval(self):\n",
        "\n",
        "        for l in self.layers:\n",
        "            l.train_flag = False\n",
        "\n",
        "    def pred(self,x):\n",
        "        return np.argmax(self(x),axis=-1)\n",
        "\n",
        "\n",
        "class AdamW:\n",
        "\n",
        "    def __init__(self,ps,lr=0.1,beta1=0.2,beta2=0.9,weight_decay=0.1):\n",
        "\n",
        "        self.ps =ps\n",
        "        self.cache = (lr,beta1,beta2,weight_decay)\n",
        "        self.hs = [\n",
        "            zeros_ps(self.ps[0]),\n",
        "            zeros_ps(self.ps[0])\n",
        "        ]\n",
        "        self.cnt = 0\n",
        "\n",
        "    def __call__(self):\n",
        "\n",
        "        eps = 1e-6\n",
        "        ps,gs = self.ps\n",
        "        ms,vs = self.hs\n",
        "        lr,b1,b2,w = self.cache\n",
        "        self.cnt += 1\n",
        "        n = self.cnt\n",
        "\n",
        "        for p,g,m,v in zip(ps,gs,ms,vs):\n",
        "\n",
        "            m[...] = b1*m + (1-b1)*g\n",
        "            v[...] = b2*v + (1-b2)*g*g\n",
        "\n",
        "            m0 = m / (1-b1**n)\n",
        "            v0 = v / (1-b2**n)\n",
        "\n",
        "            p -= w*lr*p\n",
        "\n",
        "            p -= lr*m0 / (np.sqrt(v0)+eps)\n",
        "\n",
        "\n",
        "def trainer(model,optimizer,data,epochs=200):\n",
        "\n",
        "    loss = []\n",
        "\n",
        "    for i in range(epochs):\n",
        "\n",
        "        ls = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for x,t,in  data:\n",
        "\n",
        "            y = model(x)\n",
        "            dout,l = softmax(y,t)\n",
        "            ls += l\n",
        "            model.backward(dout)\n",
        "            optimizer()\n",
        "\n",
        "        loss += [ls/len(data)]\n",
        "        model.eval()\n",
        "        if i % 10 == 0:\n",
        "            print(f'epochs = {i} loss = {loss[-1]}')\n",
        "    return loss\n",
        "\n",
        "\n",
        "def disp_loss(loss):\n",
        "\n",
        "    plt.title('Loss Function')\n",
        "    plt.xlabel('epochs')\n",
        "    plt.ylabel('cross entropy')\n",
        "    plt.plot(loss)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def disp_test(model,data,size=3):\n",
        "\n",
        "    img,_= data\n",
        "    pred = model.pred(img)\n",
        "\n",
        "    for i in range(9):\n",
        "        plt.subplot(3,3,i+1)\n",
        "        plt.axis('off')\n",
        "        plt.title(pred[i])\n",
        "        plt.imshow(img[i].reshape(8,8),'Greys')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "digits = load_digits()\n",
        "\n",
        "\n",
        "data_set = dataset(digits,0,1500)\n",
        "test_data  = dataset(digits,1500,100)\n",
        "\n",
        "\n",
        "data = DataLoader(data_set,batch_size=100)\n",
        "\n",
        "\n",
        "d_in = 64\n",
        "d_h = 4*d_in\n",
        "d_out = 10\n",
        "\n",
        "layers = [\n",
        "    Linear(d_in,d_h),\n",
        "    BatchNorm(d_h),\n",
        "    ReLU(),\n",
        "    Dropout(drop_rate=0.01),\n",
        "    Linear(d_h,d_out)\n",
        "]\n",
        "model = Layers(layers)\n",
        "optimizer = AdamW(model.ps,lr=0.01,weight_decay=0.01)\n",
        "loss = trainer(model,optimizer,data,epochs=50)\n",
        "\n",
        "#disp_loss(loss)\n",
        "disp_test(model,test_data,size=3)\n",
        "\n",
        "\n"
      ]
    }
  ]
}